# Databricks notebook source
# MAGIC %pip install geoip2 netaddr --quiet

# COMMAND ----------

from pyspark import SparkFiles, SparkContext
import geoip2.errors
from geoip2 import database
import os
import re
import shutil
import tempfile
from typing import Dict, Any, Union, Optional  
from abc import abstractmethod, ABC

from netaddr import IPAddress, AddrFormatError
from pyspark.sql import Column
import pyspark.sql.functions as F
from pyspark.sql import DataFrame
import pandas as pd

def is_public_ip(s: str) -> Optional[bool]:
    """
    Check if the given string is IP and belongs to public IPs (not loopback, private, etc.).
    Primarily it's used to avoid lookup of MaxMind database for data that doesn't exist
    :param s: string to check
    :return: true if string is global IP address
    """
    if s:
        try:
            return IPAddress(s).is_global()
        except (ValueError, AddrFormatError):
            pass

    return None

def generate_ip_range_condition(cl: Column, tmpl: str, range_start, range_end) -> Column:
    """
    Generates condition that checks if IP address is in the specified range
    :param cl: column with IP address
    :param tmpl: template for IP address range
    :param range_start: start of the range
    :param range_end: end of the range
    :return: condition
    """
    cond = None
    for i in range(range_start, range_end + 1):
        new_cond = F.startswith(cl, F.lit(tmpl.format(i)))
        if cond is None:
            cond = new_cond
        else:
            cond = cond | new_cond

    return cond

class EnrichmentBase(ABC):
    """Base class for data enrichment.  It could be used for data enrichment using functions, or
    for example, by joining with another dataframe
    """

    __not_found_marker__ = "NotFound"

    def __init__(self, name: str):
        self._name = name

    @abstractmethod
    def enrich(self, df: DataFrame) -> DataFrame:
        """Performs enrichment of the given DataFrame

        :param df: DataFrame to enrich
        :return: enriched DataFrame
        """
        pass

class ColumnEnrichment(EnrichmentBase):
    """Class that enriches DataFrame with a single column
    """
    def __init__(self, name: str, alias: Optional[str] = None):
        super().__init__(name)
        self._alias = alias

    @abstractmethod
    def get_column(self, src: Optional[str] = None, alias: Optional[str] = None) -> Column:
        """Implementation need to provide this method by returning a Column object that
        performs enrichment of the data

        :return:
        """
        pass

    def enrich(self, df: DataFrame) -> DataFrame:
        """Enriches DataFrame by adding a new column with data generated by specified function.
        The main drawback of this implementation is that it won't expand data if column is struct
        :param df: DataFrame to enrich
        :return: new DataFrame with added column

        """
        cl = self.get_column()
        if self._alias:
            cl = cl.alias(self._alias)
        return df.select("*", cl)

class PandasFunctionEnrichmentBase(ColumnEnrichment):
    """Base class for all enrichment implementations based on the Pandas UDFs
    """
    def __init__(self, name: str, src_column_name_or_expr: str, dest_column_name: str):
        """
        Initializer
        :param name: name of implementation
        :param src_column_name_or_expr: name of the source column or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """
        super().__init__(name)
        self._src_column_name_or_expr = src_column_name_or_expr
        self._dest_column_name = dest_column_name

    @abstractmethod
    def create_pandas_udf_function(self):
        """Implementations need to override this function with a function that will return
         UDF/Pandas UDF that will perform actual enrichment
        """
        pass

    def get_column(self, src: Optional[str] = None, alias: Optional[str] = None) -> Column:
        if not src:
            src = self._src_column_name_or_expr
        if not alias:
            alias = self._dest_column_name
        return self.create_pandas_udf_function()(F.expr(src)).alias(alias)

class MaxMindEnrichmentBase(PandasFunctionEnrichmentBase):
    """Base class for all enrichment implementations based on the MaxMind databases
    """

    def __init__(self, name: str, db_file: str, ip_column_name_or_expr: str, dest_column_name):
        """
        Initializer
        :param name: implementation name
        :param db_file: path to file with MaxMind database (it could be a local file, file on WSFS or Volumes)
        :param ip_column_name_or_expr: name of the column with IP information or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """
        super().__init__(name, ip_column_name_or_expr, dest_column_name)
        idx = db_file.rfind("/")
        if idx == -1:
            raise Exception(f"Please specify correct file name! got '{db_file}'")

        if db_file.startswith("dbfs:/"):
            self._dbfs_file_name = "/dbfs" + re.sub(r"^dbfs:(.*)$", r"\1", db_file)
        else:
            self._dbfs_file_name = db_file

        self._file_name = db_file[(idx + 1):]

        self._ip_column_name_or_expr = ip_column_name_or_expr
        self._dest_column_name = dest_column_name
        self.__local_tmp_directory__ = tempfile.gettempdir()

    def _copy_db_file(self, local_path):
        fd, tmp_name = tempfile.mkstemp(dir=self.__local_tmp_directory__)
        os.close(fd)
        shutil.copy2(self._dbfs_file_name, tmp_name)
        os.rename(tmp_name, local_path)

    def _get_database(self):
        """
        Returns a file name to read database from.
        :return: database file name
        """
        db_name = self._dbfs_file_name
        try:
            local_path = os.path.join(self.__local_tmp_directory__, self._file_name)
            if not os.path.exists(local_path):
                # print("No local copy found, copying")
                os.makedirs(self.__local_tmp_directory__, exist_ok=True)
                self._copy_db_file(local_path)
            else:
                # print("Local copy is older than remote copy, copying new version")
                lstat = os.stat(local_path)
                rstat = os.stat(self._dbfs_file_name)
                if rstat.st_mtime > lstat.st_mtime:
                    self._copy_db_file(local_path)
                db_name = local_path
        except OSError as e:
            print(f"OS error occurred, using remote file directly: {e}")

        return database.Reader(db_name)

    @abstractmethod
    def create_pandas_udf_function(self):
        """Implementations need to override this function with a function that will return
         UDF/Pandas UDF that will perform actual enrichment
        """
        pass

    @abstractmethod
    def __type__for_null__(self) -> str:
        pass

    def get_column(self, src: Optional[str] = None, alias: Optional[str] = None) -> Column:
        if not src:
            src = self._src_column_name_or_expr
        if not alias:
            alias = self._dest_column_name

        cl = F.expr(src)
        # TODO: add more checks for private IP addresses (RFC 3330)
        skip_cond = (cl.isNull() | (cl == "") | (~F.contains(cl, F.lit(".")) & ~F.contains(cl, F.lit(":"))) |
                     F.startswith(cl, F.lit("127.")) | F.startswith(cl, F.lit("192.168.")) |
                     F.startswith(cl, F.lit("10.")) | F.startswith(cl, F.lit("169.254.")) |
                     generate_ip_range_condition(cl, "172.{}.)", 16, 31))
        new_cl = (F.when(skip_cond, F.lit(None).cast(self.__type__for_null__())).otherwise(
            self.create_pandas_udf_function()(cl))).alias(alias)

        return new_cl


class GeoIPEnrichment(MaxMindEnrichmentBase):
    """Class that enriches DataFrame with GeoIP data
    """
    EMPTY_RECORD = {'city': None, 'country': None, 'country_code': None,
                    'latitude': None, 'longitude': None, 'accuracy_radius': None}

    def __init__(self, db_file: str, ip_column_name_or_expr: str, dest_column_name: str = "geo"):
        super().__init__("GeoIP", db_file, ip_column_name_or_expr, dest_column_name)
        """

        :param db_file: path to file with MaxMind database
        :param ip_column_name_or_expr: name of the column with IP information or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """

    def __type__for_null__(self):
        return ("struct<city:string, country:string, country_code:string, latitude:double, "
                "longitude:double, accuracy_radius:int>")

    def create_pandas_udf_function(self):
        def extract_geoip_data(ip: str, geocity, cache: Dict[str, Union[str, Dict[str, Any]]]):
            cv = cache.get(ip)
            if cv is not None:
                if isinstance(cv, str):
                    return None
                return cv
            rc: Union[str, Dict[str, Any]] = GeoIPEnrichment.EMPTY_RECORD
            if ip and is_public_ip(ip):
                try:
                    record = geocity.city(ip)
                    rc = {
                        'city': record.city.name,
                        'country': record.country.name,
                        'country_code': record.country.iso_code,
                        'latitude': record.location.latitude,
                        'longitude': record.location.longitude,
                        'accuracy_radius': record.location.accuracy_radius
                    }
                except (geoip2.errors.AddressNotFoundError, ValueError):
                    pass

            cache[ip] = rc

            return rc

        @F.pandas_udf(
            "city string, country string, country_code string, latitude double, longitude double, accuracy_radius int")
        def get_geoip_data(ips: pd.Series) -> pd.DataFrame:
            geocity = self._get_database()
            cache: Dict[str, Union[str, Dict[str, Any]]] = {}
            extracted = ips.apply(lambda ip: extract_geoip_data(ip, geocity, cache))

            return pd.DataFrame(extracted.values.tolist())

        return get_geoip_data
    
class ASNEnrichment(MaxMindEnrichmentBase):
    """Class that enriches DataFrame with data about Autonomous System (AS)
    """
    EMPTY_RECORD = {'as_number': None, 'as_org': None, 'as_network': None}

    def __init__(self, db_file: str, ip_column_name_or_expr: str, dest_column_name: str = "as_data"):
        super().__init__("ASN", db_file, ip_column_name_or_expr, dest_column_name)
        """

        :param db_file: path to file with MaxMind database
        :param ip_column_name_or_expr: name of the column with IP information or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """

    def __type__for_null__(self):
        return "struct<as_number:int, as_org:string, as_network:string>"

    def create_pandas_udf_function(self):
        def extract_asn_data(ip: str, asn, cache: Dict[str, Union[str, Dict[str, Any]]]):
            cv = cache.get(ip)
            if cv is not None:
                if isinstance(cv, str):
                    return None
                return cv
            rc: Union[str, Dict[str, Any]] = ASNEnrichment.EMPTY_RECORD
            if ip and is_public_ip(ip):
                try:
                    record = asn.asn(ip)
                    rc = {'as_number': record.autonomous_system_number,
                          'as_org': record.autonomous_system_organization,
                          'as_network': str(record.network)}
                except (geoip2.errors.AddressNotFoundError, ValueError):
                    pass

            cache[ip] = rc

            return rc

        @F.pandas_udf("as_number int, as_org string, as_network string")
        def get_asn_data(ips: pd.Series) -> pd.DataFrame:
            asn = self._get_database()
            cache: Dict[str, Union[str, Dict[str, Any]]] = {}
            extracted = ips.apply(lambda ip: extract_asn_data(ip, asn, cache))

            return pd.DataFrame(extracted.values.tolist())

        return get_asn_data

geo_enricher = GeoIPEnrichment(
  '/Volumes/derek/sirens/sample_data/GeoLite2_City.mmdb', 
  ip_column_name_or_expr='geo'
)
asn_enricher = ASNEnrichment(
    '/Volumes/derek/sirens/sample_data/GeoLite2_ASN.mmdb',
    ip_column_name_or_expr=None
)
geo_info = geo_enricher.create_pandas_udf_function()
asn_info = asn_enricher.create_pandas_udf_function()

# COMMAND ----------

import functools
from enum import Enum
import uuid
from pyspark.sql.functions import col, lit, to_timestamp, to_json, struct, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from delta.tables import DeltaTable

class Output(Enum):
    asDataFrame = "dataframe"
    asAlert = "alert"

alerts_schema = StructType([
    StructField("alert_id", StringType(), False),
    StructField("alertTime", TimestampType(), True),
    StructField("eventTime", TimestampType(), True),
    StructField("user_email", StringType(), True),
    StructField("event_type", StringType(), False),
    StructField("source_ip", StringType(), True),
    StructField("event_data", StringType(), True)  # JSON string
])

def _alerts_df(df):
    alerts_df = df.select(
        lit(str(uuid.uuid4())).alias("alert_id"),
        current_timestamp().alias("alertTime"),
        to_timestamp(col("EVENT_DATE")).alias("eventTime"),
        col("SRC_USER").alias("user_email"),
        col("ACTION").alias("event_type"),
        col("SRC_IP").alias("source_ip"),
        to_json(struct([col(c) for c in df.columns])).alias("event_data")
        ).na.fill({"user_email": "unknown", "source_ip": "unknown"}
    )
    
    return alerts_df

def _write_alerts(df, alerts_table_path: str):
    
    delta_table = DeltaTable.forPath(spark, alerts_table_path)
    delta_table.alias("target").merge(
        df.alias("source"),
        "target.alert_id = source.alert_id"
        ).whenNotMatchedInsertAll().execute()
    return


def detect(func=None, *, output=None, columns=None, arg=None):
    """Decorator function with arguments
    Decorator can be used with or without arguments
    Examples:
        >>>
        >>> @detect
        >>> def func():
        >>>     pass
        >>>
        >>> @detect(arg='foo')
        >>> def func():
        >>>     pass
        >>>
    """
    # 1. Decorator arguments are applied to itself as partial arguments
    if func is None:
        return functools.partial(detect, output=output, columns=columns, arg=arg)

    # 2. logic with the arguments
 
    # 3. Handles the actual decorating
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Write decorator function logic here
        # Before function call
        # ...
        result = func(*args, **kwargs)
        # After function call
        # ...
        if output == Output.asAlert:
            result = _alerts_df(result)
    

        return result
    return wrapper